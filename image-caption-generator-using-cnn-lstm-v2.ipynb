{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import Modules","metadata":{}},{"cell_type":"code","source":"import os  # handling the files\nimport pickle  # storing numpy features, like image features\nimport numpy as np\nfrom tqdm.notebook import tqdm  # UI to show how much data has been processed\n\nfrom tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array  # loading and preprocessing the images\nfrom tensorflow.keras.preprocessing.text import Tokenizer # preprocessing the text\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences  # to even out the lengths of the sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import to_categorical, plot_model  # interpretation of the model in terms of an image\nfrom tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\n\nfrom nltk.translate.bleu_score import corpus_bleu\n\nfrom PIL import Image  # to load the image\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T15:10:11.764646Z","iopub.execute_input":"2024-12-04T15:10:11.765019Z","iopub.status.idle":"2024-12-04T15:10:11.770504Z","shell.execute_reply.started":"2024-12-04T15:10:11.764987Z","shell.execute_reply":"2024-12-04T15:10:11.769686Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BASE_DIR = '/kaggle/input/flickr8k'\nWORKING_DIR = '/kaggle/working'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T15:10:12.957712Z","iopub.execute_input":"2024-12-04T15:10:12.958035Z","iopub.status.idle":"2024-12-04T15:10:12.962092Z","shell.execute_reply.started":"2024-12-04T15:10:12.958008Z","shell.execute_reply":"2024-12-04T15:10:12.961084Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load VGG16 model\nmodel = VGG16()\n\n# Restructure the model\n# Why? - We don't need the fully connected layer of the VGG16 model. We just need last second layer to extract the features\n# Note - The last layer is predictions (Dense), which we don't need\n# The one we're using is last second layer - fc2 (Dense)\nmodel = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n\n# Summarize\nprint(model.summary())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T15:10:14.567608Z","iopub.execute_input":"2024-12-04T15:10:14.568404Z","iopub.status.idle":"2024-12-04T15:10:16.814952Z","shell.execute_reply.started":"2024-12-04T15:10:14.568372Z","shell.execute_reply":"2024-12-04T15:10:16.814101Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extract Features from Image using a dictionary\n# key = image ID, value = features\nfeatures = {}\ndirectory = os.path.join(BASE_DIR, 'Images')\n\nfor img_name in tqdm(os.listdir(directory)):\n    # Load the image from file\n    img_path = directory + '/' + img_name\n    image = load_img(img_path, target_size=(224, 224))  # images will get resized to 224*224\n    # Convert image pixels to numpy array\n    image = img_to_array(image)\n    # Reshape data for model, in order to extract features\n    # shape[i] represents r, g, b values for i = 0, 1, 2\n    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n    # Preprocess image for VGG\n    image = preprocess_input(image)\n    # Extract features\n    feature = model.predict(image, verbose=0)  # verbose=0 => it won't display any text or status\n    # Get image ID\n    # image_id's look like blahblah.jpg\n    # We just need the blahblah part, so take [0] of the image_id\n    image_id = img_name.split('.')[0]\n    # Store features\n    features[image_id] = feature","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T15:10:24.931300Z","iopub.execute_input":"2024-12-04T15:10:24.931644Z","iopub.status.idle":"2024-12-04T15:19:19.537365Z","shell.execute_reply.started":"2024-12-04T15:10:24.931613Z","shell.execute_reply":"2024-12-04T15:19:19.536278Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Store Extracted Features in a File\npickle.dump(features, open(os.path.join(WORKING_DIR, 'features.pkl'), 'wb'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T15:23:32.296043Z","iopub.execute_input":"2024-12-04T15:23:32.296634Z","iopub.status.idle":"2024-12-04T15:23:32.565586Z","shell.execute_reply.started":"2024-12-04T15:23:32.296601Z","shell.execute_reply":"2024-12-04T15:23:32.564913Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load Features from Pickle\nwith open(os.path.join(WORKING_DIR, 'features.pkl'), 'rb') as f:\n    features = pickle.load(f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T15:23:34.945313Z","iopub.execute_input":"2024-12-04T15:23:34.945646Z","iopub.status.idle":"2024-12-04T15:23:35.100457Z","shell.execute_reply.started":"2024-12-04T15:23:34.945616Z","shell.execute_reply":"2024-12-04T15:23:35.099517Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load the Captions Data","metadata":{}},{"cell_type":"code","source":"with open(os.path.join(BASE_DIR, 'captions.txt'), 'rb') as f:\n    # The first line of the captions.txt file is \"image, text\". We don't need this line, so use next(f) to skip that line\n    next(f)\n    captions_doc = f.read()  # Reads the whole text data from captions.txt\n# Note - The captions_doc has files in \"1000268201_693b08cb0e.jpg,A child in a pink dress is climbing up a set of stairs in an entry way .\" format","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T15:23:38.565252Z","iopub.execute_input":"2024-12-04T15:23:38.565598Z","iopub.status.idle":"2024-12-04T15:23:38.692383Z","shell.execute_reply.started":"2024-12-04T15:23:38.565560Z","shell.execute_reply":"2024-12-04T15:23:38.691509Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create Mapping of Images to Captions\nmapping = {}\n# Process Lines\nfor line in tqdm(captions_doc.split('\\n')):  # By splitting it by /n, you're iterating line-by-line\n    # Split the Line by Commas(,)\n    tokens = line.split(',')  # Commas act as separators\n    # If line = img_01, 'A cute panda', 'A chonky panda'\n    # tokens = ['img_01', 'A cute panda', 'A chonky panda']\n    # Basically, the split function is separating the image ID, and the caption sentence\n    \n    if len(line) < 2:\n        continue  # Small lines might give an error, so avoid the error by continuing\n    image_id, caption = tokens[0], tokens[1:]\n    # Remove .jpg extension from image_id\n    image_id = image_id.split('.')[0]\n    # Convert caption list to strings\n    # The following line will concatenate the caption as,\n    # caption = 'A cute panda A chonky panda'\n    caption = \" \".join(caption)\n    \n    # An image can have multiple captions\n    # Create a list of captions, if needed\n    if image_id not in mapping:\n        mapping[image_id] = []\n    # Store the caption corresponding to the image\n    mapping[image_id].append(caption)\n    # mapping[img_01] = 'A cute panda A chonky panda'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T15:29:58.469090Z","iopub.execute_input":"2024-12-04T15:29:58.469625Z","iopub.status.idle":"2024-12-04T15:29:58.727061Z","shell.execute_reply.started":"2024-12-04T15:29:58.469566Z","shell.execute_reply":"2024-12-04T15:29:58.724994Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Number of images we have\nlen(mapping)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T15:23:13.185485Z","iopub.execute_input":"2024-12-04T15:23:13.185845Z","iopub.status.idle":"2024-12-04T15:23:14.146713Z","shell.execute_reply.started":"2024-12-04T15:23:13.185815Z","shell.execute_reply":"2024-12-04T15:23:14.145739Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Pre-process the Captions\n# caption = 'A cute panda A chonky panda'\ndef clean(mapping):\n    for key, captions in mapping.items():\n        for i in range(len(captions)):\n            # Take one caption at a time\n            caption = captions[i]\n            # Pre-processing steps - \n            caption = caption.lower()  # Convert everything to lower case\n            caption = caption.replace('[^A-Za-z]', '')  # Remove all special characters, numbers, etc.\n            caption = caption.replace('\\s+', ' ')  # Replace multiple spaces with a single space\n            # Add start and end tags to the caption\n            # Helps the model decide when to start and when to stop\n            # Also, remove single character words like 'a', we don't need it\n            # 'A girl is walking' ~= 'girl is walking'\n            caption = 'startseq' + \" \".join([word for word in caption.split() if len(word)>1]) + 'endseq'\n            # Replace the original captions with the pre-processed ones\n            captions[i] = caption\n'''\nAfter using this function for the mapping['img_01] = 'A cute panda A chonky panda', \nit will become:\nmapping['img_01] = ['startseq cute panda endseq', 'starseq chonky panda endseq']\nNOTE : Earlier I had used <start> and <end> to denote the starting and ending of a sentence, but later the special characters get removed anyway, so I changed those names to startseq and endseq.\n'''\n            ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Store all captions in a single list\nall_captions = []\nfor key in mapping:\n    for caption in mapping[key]:\n        all_captions.append(caption)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check length of captions to get an idea\nprint(len(all_captions))\nprint(all_captions[:10])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Tokenize the text\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(all_captions)\nvocab_size = len(tokenizer.word_index) + 1  # Total number of unique words you're having = 8483 in this case\nvocab_size","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get maximum length of the caption available\n# Will be useful for padding later on\nmax_length = max(len(caption.split()) for caption in all_captions)  # = 35 in this case\nmax_length","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Train-test Split","metadata":{}},{"cell_type":"code","source":"# Train Test Split\nimage_ids = list(mapping.keys())\nsplit_ratio = int(len(image_ids) * 0.90)  # 90% of the data for training\ntrain = image_ids[:split]\ntest = image_ids[split:]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Kaggle has maximum 13GB of RAM\n# If you feed the entire thing directly, it'll crash\n# Create data generator to divide data into batches and get it, because we have limited memory\n\ndef data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n    # Loop over images\n    X1, X2, y = list(), list(), list()\n    n = 0  # To determine whether we reach the batch size or not\n    while 1:  # Infinite loop\n        for key in data_keys:  # data_keys are the image_id's of the training data\n            # When you get new data, increment n\n            n += 1\n            captions = mapping[key]\n            # Process each caption\n            for caption in captions:\n                # Encode the sequence\n                # For each word, assign an index\n                seq = tokenizer.texts_to_sequences([caption])[0]\n                # Split the sequence into X (input), y(output) pairs\n                for i in range(1, len(seq)):\n                    # Split into input and output data\n                    in_seq, out_seq = seq[:i], seq[i]\n                    # Pad the input sequence\n                    # There will be additional results/outputs too\n                    # So you should get only the [0]th term\n                    in_seq = pad_sequence([in_seq], maxlen=max_length)[0]\n                    # Encode output sequence into one-hot encoding\n                    # One-hot encoding - If the word is present, it'll be represented as 1, otherwise 0\n                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n\n                    # Store the sequences\n                    X1.append(features[key][0])  # Image features\n                    X2.append(in_seq)  # Text features\n                    y.append(out_seq)  # Output is the out_seq\n            if n == batch_size:\n                # Convert into np arrays, because model can't process normal python lists\n                X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n                # Return the collected sequences for the model to consume\n                yield [X1, X2], y\n                # Reinitialize X1, X2, y, because we no longer need the values from the last iteration\n                # If you don't reinitialize, you'll exhaust the memory\n                X1, X2, y = list(), list(), list()\n                n = 0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"What the code above does is:\n\nSay we have a caption: startseq girl going into wooden building endseq\nFor each iteration, there will be new X and y\nFor first iteration, X=startseq, and the model predicts y=girl. For the second iteration, X=startseq girl, and the model is supposed to predict y=going.\nX is input sequence.\n\nThis is how the sequence flow works","metadata":{}},{"cell_type":"code","source":"'''\n       X                                             y\n    startseq                                         girl\n    startseq girl                                    going\n    startseq girl going                              inside\n    startseq girl going inside                       wooden \n    startseq girl going inside wooden                building\n    startseq girl going inside wooden building       endseq\n    startseq girl going inside wooden building endseq\n\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T08:35:35.702293Z","iopub.execute_input":"2024-12-05T08:35:35.702953Z","iopub.status.idle":"2024-12-05T08:35:35.731759Z","shell.execute_reply.started":"2024-12-05T08:35:35.702911Z","shell.execute_reply":"2024-12-05T08:35:35.730556Z"}},"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"'\\n       X                                             y\\n    <start>                                         girl\\n    <start> girl                                    going\\n    <start> girl going                              inside\\n    <start> girl going inside                       wooden \\n    <start> girl going inside wooden                building\\n    <start> girl going inside wooden building       <end>\\n    <start> girl going inside wooden building <end>\\n\\n'"},"metadata":{}}],"execution_count":1},{"cell_type":"markdown","source":"## Model Creation","metadata":{}},{"cell_type":"code","source":"# Encoder model :\n\n# Image-feature layers\ninputs1 = Input(shape=(4096,))  # is the input for fe1 layer\nfe1 = Dropout(0.4)(inputs1)  # feature 1, which is the input for fe2 layer\nfe2 = Dense(256, activation='relu')(fe2)\n\n# Sequence-feature layers\ninputs2 = Input(shape=(max_length,))\nse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)  # mask_zero=True, because we're padding the sequence\nse2 = Dropout(0.4)(se1)\nse3 = LSTM(256)(se2)\n\n# Decoder model : \ndecoder1 = add([fe2, se3])  # Concatenate the features\ndecoder2 = Dense(256, activation='relu')(decoder1)\noutputs = Dense(vocab_size, activation='softmax')(decoder2)\n\n# Model with 2 inputs, and 1 output\nmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\n# Loss and optimizer\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\n# Plot the model\nplot_model[model, show_shapes=True]  # You can even save this as an image","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Since we had already extracted the features using VGG, we didn't use any CNNs in the code snippet for the layers above","metadata":{}},{"cell_type":"code","source":"# Train the Model\n# Using data_generator function => model will train slowly, but will take less RAM\n# Not using data_encoder function => model trains faster, but takes a lot of memory -> might crash\nepochs = 15\nbatch_size = 64\nsteps = len(train) // batch_size  # After each step, it does back-propagation, and fetches the data\n\nfor i in range(epochs):\n    # Create data generator\n    generator = data_generator(train, mapping, features, tokenizer, max_length, vocab_size, batch_size)\n    # From the generator, you'll get your inputs - X1 (image features), X2 (sequence features), y (outputs or labels)\n    # Fit for one epoch\n    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n    # This won't show any validation accuracy, because we don't have any validation generator\n    # We'll do testing and validation after training the model\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the trained model\nmodel.save(WORKING_DIR+'/best_model.h5')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Generate Captions for the Image","metadata":{}},{"cell_type":"code","source":"# Convert id into a word\n# The output of the model will be just indices\n# Convert those indices into words\n\n# Helper function\ndef idx_to_word(integer, tokenizer):\n    for word, index in tokenizer.word_index.items():\n        if index==integer:\n            return word\n    return None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate caption for an image\ndef predict_caption(model, image, tokenizer, max_length):\n    # Add start tag for generation process\n    in_text = 'startseq'\n    # Iterate over the max length of the sequence (=35, here)\n    for i in range(max_length):\n        # Encode input sequence. Convert Sequence -> Integer\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        # Pad the sequence\n        sequence = pad_sequences([sequence], max_length)\n        # Predict next word\n        # yhat will be probabilities\n        yhat = model.predict([image, sequence], verbose=0)\n        # Choose the word with highest probability\n        yhat = np.argmax(yhat)  # Gives index with max probability\n        # Convert index to word\n        word = idx_to_word(yhat, tokenizer)\n        # Stop if word not found\n        if word is None:\n            break\n        # Append word as input for generating next word in the caption\n        in_text += \" \" + word\n        # Stop if we reach <end> tag\n        if word == 'endseq':\n            break\n    return in_text  # Return the caption","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Validate with Text Data\nactual, predicted = list(), list()\nfor key in tqdm(test):\n    # Get actual caption\n    captions = mappingp[key]\n    # Predict the caption for image\n    # y_pred will have a text-like caption in it\n    y_pred = predict_caption(model, feature[key], tokenizer, max_length)  # Pass the trained model, image features\n    # Split into words\n    actual_captions = [caption.split() for caption in captions]\n    y_pred = y_pred.split()\n    # Append to the list\n    actual.append(actual_captions)\n    predicted.append(y_pred)\n\n# Calculate BLEU Score - Score we need to consider when we have text data\n# Higher the score, the better. Ranges from 0-1\n# Increase number of epochs and see if the score improves\nprint(\"BLEU-1: %f\" % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\nprint(\"BLEU-2: %f\" % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualize the Results","metadata":{}},{"cell_type":"code","source":"def generate_caption(image_name):\n    # Load the Image\n    # image_name = \"1009434119_febe49276a.jpg\"\n    image_id = image_name.split('.')[0]\n    img_path = os.path.join(BASE_DIR, \"Images\", image_name)\n    image = Image.open(img_path)\n    captions = mapping[image_id]\n    print('------------------ Actual ------------------')\n    for caption in captions:\n        print(caption)\n    y_pred = predict_caption(model, features[image_id], tokenizer, max_length)\n    print('--------------------- Predicted ------------------')\n    print(y_pred)\n    plt.imshow(image)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generate_caption('009434119_febe49276a.jpg')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}